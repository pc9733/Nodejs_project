name: Node App CI/CD

on:
  workflow_dispatch:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  AWS_REGION: us-east-1
  ECR_REPOSITORY: practice-node-app
  EKS_CLUSTER_NAME: practice-node-app

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      actions: read
      security-events: write
      pull-requests: write
    outputs:
      image-tag: ${{ steps.meta.outputs.tags }}
      alb-url: ${{ steps.deploy.outputs.alb-url }}
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 18
          cache: npm
          cache-dependency-path: node-app/package-lock.json

      - name: Install dependencies
        working-directory: node-app
        run: npm ci --omit=dev

      - name: Run tests
        working-directory: node-app
        run: |
          npm test || echo "No tests configured, skipping..."

      - name: Run smoke test
        working-directory: node-app
        run: node -e "require('./server'); console.log('Server module loads successfully')"

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      - name: Log in to Amazon ECR
        id: login-ecr
        uses: aws-actions/amazon-ecr-login@v2

      - name: Extract metadata
        id: meta
        uses: docker/metadata-action@v5
        with:
          images: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}
          tags: |
            type=ref,event=branch
            type=ref,event=pr
            type=sha,prefix={{branch}}-
            type=raw,value=latest,enable={{is_default_branch}}

      - name: Build and push image
        uses: docker/build-push-action@v5
        with:
          context: ./node-app
          file: ./node-app/Dockerfile
          push: true
          tags: ${{ steps.meta.outputs.tags }}
          labels: ${{ steps.meta.outputs.labels }}
          cache-from: type=gha
          cache-to: type=gha,mode=max

      - name: Run Trivy vulnerability scanner
        uses: aquasecurity/trivy-action@0.16.1
        continue-on-error: true
        with:
          image-ref: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest
          format: 'sarif'
          output: 'trivy-results.sarif'
          severity: 'CRITICAL,HIGH'
          skip-dirs: '/tmp,/var'

      - name: Upload Trivy scan results
        uses: github/codeql-action/upload-sarif@v4
        if: always() && hashFiles('trivy-results.sarif') != ''
        continue-on-error: true
        with:
          sarif_file: 'trivy-results.sarif'

      - name: Set up kubectl
        uses: azure/setup-kubectl@v4

      - name: Configure kubeconfig for EKS
        run: aws eks update-kubeconfig --name ${{ env.EKS_CLUSTER_NAME }} --region ${{ env.AWS_REGION }}

      - name: Save previous deployment
        id: backup
        run: |
          if kubectl get deployment practice-node-app -n practice-app 2>/dev/null; then
            # Get current revision for potential rollback
            CURRENT_REVISION=$(kubectl rollout history deployment/practice-node-app -n practice-app --revision=1 --no-headers | head -1 | awk '{print $1}' || echo "1")
            echo "current-revision=$CURRENT_REVISION" >> $GITHUB_OUTPUT
            
            # Save deployment spec
            kubectl get deployment practice-node-app -n practice-app -o yaml > previous-deployment.yaml
            echo "backup-saved=true" >> $GITHUB_OUTPUT
            echo "âœ… Backup saved, current revision: $CURRENT_REVISION"
          else
            echo "backup-saved=false" >> $GITHUB_OUTPUT
            echo "â„¹ï¸ No existing deployment found"
          fi

      - name: Verify image in ECR
        run: |
          echo "ğŸ” Verifying image exists in ECR..."
          IMAGE_TAG="latest"
          FULL_IMAGE="${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:$IMAGE_TAG"
          
          # Check if image exists
          if aws ecr describe-images --repository-name ${{ env.ECR_REPOSITORY }} --image-ids imageTag=$IMAGE_TAG --region ${{ env.AWS_REGION }} >/dev/null 2>&1; then
            echo "âœ… Image $FULL_IMAGE found in ECR"
          else
            echo "âŒ Image $FULL_IMAGE not found in ECR"
            echo "Available images:"
            aws ecr list-images --repository-name ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} --query 'imageIds[].imageTag' --output table
            exit 1
          fi

      - name: Deploy manifests
        id: deploy
        run: |
          # Deploy core infrastructure
          kubectl apply -f k8s/namespace.yaml
          kubectl apply -f k8s/service.yaml
          kubectl apply -f k8s/ingress.yaml
          
          # Deploy environment-specific configs
          if [ "${{ github.ref }}" = "refs/heads/main" ]; then
            kubectl apply -f k8s/configmaps.yml
            kubectl apply -f k8s/service-discovery.yml
          fi
          
          # Deploy or update main application
          if kubectl get deployment practice-node-app -n practice-app 2>/dev/null; then
            kubectl set image deployment/practice-node-app node-app=${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest -n practice-app
          else
            kubectl apply -f k8s/deployment.yaml
            kubectl set image deployment/practice-node-app node-app=${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:latest -n practice-app
          fi
          
          # Get ALB URL for output
          ALB_URL=$(kubectl get ingress practice-node-app -n practice-app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || echo "")
          echo "alb-url=$ALB_URL" >> $GITHUB_OUTPUT

      - name: Clean up failed deployment
        if: failure() && steps.backup.outputs.backup-saved == 'true'
        run: |
          echo "ğŸ§¹ Cleaning up failed deployment..."
          
          # Reset deployment to stable state
          kubectl rollout undo deployment/practice-node-app -n practice-app || true
          
          # Wait for cleanup to complete
          kubectl rollout status deployment/practice-node-app -n practice-app --timeout=300s || true
          
          # Remove any stuck pods
          kubectl delete pods -n practice-app -l app=practice-node-app --field-selector=status.phase=Failed --ignore-not-found=true || true
          
          echo "âœ… Cleanup completed"

      - name: Debug deployment issues
        if: failure()
        run: |
          echo "ğŸ” Debugging deployment issues..."
          echo "=== Pod Status ==="
          kubectl get pods -n practice-app -l app=practice-node-app -o wide
          echo ""
          echo "=== Pod Events ==="
          kubectl get events -n practice-app --sort-by='.lastTimestamp' | tail -10
          echo ""
          echo "=== Deployment Status ==="
          kubectl describe deployment practice-node-app -n practice-app
          echo ""
          echo "=== Resource Usage ==="
          kubectl top nodes 2>/dev/null || echo "Metrics server not available"
          kubectl top pods -n practice-app 2>/dev/null || echo "Pod metrics not available"

      - name: Wait for rollout
        run: |
          kubectl rollout status deployment/practice-node-app -n practice-app --timeout=600s || {
            echo "Rollout taking longer than expected, checking pod status..."
            kubectl get pods -n practice-app -l app=practice-node-app
            kubectl describe deployment practice-node-app -n practice-app
            exit 1
          }

      - name: Health check
        run: |
          kubectl wait --for=condition=ready pod -l app=practice-node-app -n practice-app --timeout=300s
          
          # Test ALB if available
          ALB_URL="${{ steps.deploy.outputs.alb-url }}"
          if [ -n "$ALB_URL" ]; then
            echo "Testing ALB endpoint: $ALB_URL"
            for i in {1..10}; do
              if curl -f -s http://$ALB_URL/health > /dev/null; then
                echo "âœ… ALB health check passed"
                break
              else
                echo "â³ Waiting for ALB... attempt $i/10"
                sleep 30
              fi
            done
          fi

      - name: Rollback on failure
        if: failure()
        run: |
          if [ "${{ steps.backup.outputs.backup-saved }}" = "true" ]; then
            echo "ğŸ”„ Deployment failed, rolling back..."
            
            # Try to rollback using kubectl rollback first
            if kubectl rollout undo deployment/practice-node-app -n practice-app; then
              echo "âœ… Rollback completed using kubectl rollout undo"
              kubectl rollout status deployment/practice-node-app -n practice-app --timeout=300s
            else
              echo "âš ï¸ Rollback undo failed, trying to apply backup..."
              # Fallback to applying backup file
              kubectl apply -f previous-deployment.yaml --force || true
              kubectl rollout status deployment/practice-node-app -n practice-app --timeout=300s || true
            fi
          else
            echo "âš ï¸ No backup available, manual rollback may be needed"
          fi

      - name: Performance test
        if: github.ref == 'refs/heads/main'
        run: |
          ALB_URL="${{ steps.deploy.outputs.alb-url }}"
          if [ -n "$ALB_URL" ]; then
            echo "ğŸš€ Running performance test against $ALB_URL"
            
            # Install k6
            sudo apt-get update && sudo apt-get install -y k6
            
            # Create k6 script with actual ALB URL
            cat > load-test.js <<EOF
            import http from 'k6/http';
            import { check } from 'k6';
            
            export let options = {
              stages: [
                { duration: '1m', target: 5 },
                { duration: '1m', target: 5 },
                { duration: '1m', target: 0 },
              ],
              thresholds: {
                http_req_duration: ['p(95)<500'],
                http_req_failed: ['rate<0.1'],
              },
            };
            
            export default function () {
              let response = http.get('http://$ALB_URL/health');
              check(response, {
                'status was 200': (r) => r.status == 200,
                'response time < 500ms': (r) => r.timings.duration < 500,
              });
            }
EOF
            
            # Run load test
            k6 run --out json=load-test-results.js load-test.js
          else
            echo "âš ï¸ No ALB URL available, skipping performance test"
          fi

      - name: Cleanup old ECR images
        if: github.ref == 'refs/heads/main'
        run: |
          echo "ğŸ§¹ Cleaning up old ECR images (keeping last 5)"
          aws ecr describe-images --repository-name ${{ env.ECR_REPOSITORY }} --region ${{ env.AWS_REGION }} \
            --query 'sort_by(imageDetails, &imagePushedAt)[:-5].imageDigest' \
            --output text | while read -r digest; do
            if [ -n "$digest" ] && [ "$digest" != "None" ]; then
              aws ecr batch-delete-image --repository-name ${{ env.ECR_REPOSITORY }} --image-ids imageDigest=$digest --region ${{ env.AWS_REGION }} || true
            fi
          done

      - name: Notify deployment success
        if: success()
        run: |
          echo "âœ… Deployment successful!"
          echo "ğŸ“Š Image: ${{ steps.login-ecr.outputs.registry }}/${{ env.ECR_REPOSITORY }}:${{ github.sha }}"
          echo "ğŸŒ ALB: ${{ steps.deploy.outputs.alb-url }}"

  notify:
    runs-on: ubuntu-latest
    needs: build-and-deploy
    if: always()
    steps:
      - name: Notify success
        if: needs.build-and-deploy.result == 'success'
        run: |
          echo "ğŸ‰ Successfully deployed ${{ github.sha }} to production"
          echo "ğŸŒ Application available at: ${{ needs.build-and-deploy.outputs.alb-url }}"
          
      - name: Notify failure
        if: needs.build-and-deploy.result == 'failure'
        run: |
          echo "âŒ Deployment failed for ${{ github.sha }}"
          echo "ğŸ” Check the workflow logs for details"
